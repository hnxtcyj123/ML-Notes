# 长度外推性

Transformer模型本身是无法建模序列关系的，其本质是全局Attention加上FFN，对于序列中token的顺序建模基本依赖于位置编码。这样就导致了一个问题，如果Transformer在训练和预测时的输入序列长度不一致，其性能可能会严重下降，这就是长度外推性问题。

具体来说，不一致的地方有两点：

> 1、预测的时候用到了没训练过的位置编码（不管绝对还是相对）；
>
> 2、预测的时候注意力机制所处理的token数量远超训练时的数量。

第1点是位置编码没有得到训练所致。关于第2点，越多的token去平均注意力，意味着最后的分布相对来说越“均匀”（熵更大），即注意力越分散；而训练长度短，则意味着注意力的熵更低，注意力越集中，这也是一种训练和预测的差异性，也会影响效果。

## 1 位置编码

位置编码是一种方法，用于让模型能够区分序列中元素的顺序和相对位置。位置编码可以分为绝对位置编码和相对位置编码两大类。绝对位置编码是直接将每个位置的信息加到元素的表示上，而相对位置编码是在计算注意力分数时考虑两个元素之间的距离。

### 1.1 绝对位置编码

绝对位置编码是直接将序列中每个位置的信息编码进模型的，从而使模型能够了解每个元素在序列中的具体位置。

#### **1.1.1 sinusoidal位置编码**

原始Transformer提出时采用了sinusoidal位置编码，通过正弦和余弦的函数结构使得模型捕获位置之间的复杂关系，且这些编码与序列中每个位置的绝对值有关。


其中，pos表示位置，代表embedding的维度，代表的是embedding不同位置的索引。

原始 Transformer 的位置编码虽然是基于绝对位置的，但其数学结构使其能够捕获一些相对位置信息。使用正弦和余弦函数的组合为每个位置创建编码，波长呈几何级数排列，意味着每个位置的编码都是独特的。然而，正弦和余弦函数的周期性特性确保了不同位置之间的编码关系是连续且平滑的。

#### 1.1.2 训练式位置编码

这种方法是将每个位置的编码作为一个可学习的向量，类似于词向量。这种方法的优点是可以适应不同的任务和数据，缺点是需要额外的参数和训练时间。BERT、GPT2等模型采用了这种方法。

相对位置编码

相对位置编码

这种方法是在计算注意力分数时，直接使用两个元素之间的相对位置向量或矩阵。这种方法的优点是可以更直接地表示相对位置关系，缺点是可能增加计算复杂度和内存消耗。Transformer-XL、XLNet、DeBERTa等模型采用了这种方法。

**旋转位置编码**：这种方法是使用复数的旋转操作来给元素的表示添加位置信息，使得两个元素的内积可以表示它们之间的相对位置关系。这种方法的优点是可以用绝对位置编码来表征相对位置编码，缺点是可能存在一些数学上的限制。RoFormer、CPM等模型采用了这种方法。
